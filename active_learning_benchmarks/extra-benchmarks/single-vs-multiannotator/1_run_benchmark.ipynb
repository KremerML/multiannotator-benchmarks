{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cleanlab.multiannotator import get_majority_vote_label, get_label_quality_multiannotator, get_active_learning_scores\n",
    "from cleanlab.internal.label_quality_utils import get_normalized_entropy\n",
    "\n",
    "from utils.model_training import fit_predict_proba\n",
    "from utils.active_learning import setup_next_iter_data, setup_next_iter_data_single, add_new_annotator, add_new_annotator_single, get_idx_to_label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Files\n",
    "\n",
    "The datafiles can either be generate yourself by running [0_create_data.ipynb](0_create_data.ipynb), or by downloading our pre-generated files using the commands:\n",
    "\n",
    "```\n",
    "wget -nc 'https://cleanlab-public.s3.amazonaws.com/ActiveLearning/Benchmark/SingleVsMultiannotator/data.tar.gz'\n",
    "tar -xf data.tar.gz data/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 5\n",
    "num_rounds = 15\n",
    "batch_size_to_label = 100\n",
    "\n",
    "noise_rate_arr = [0.6, 0.7, 0.8, 0.9, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(noise_rate):\n",
    "    multiannotator_labels = pd.DataFrame(\n",
    "        np.load(f\"data/{noise_rate}/multiannotator_labels_labeled.npy\")\n",
    "    )\n",
    "    single_labels = np.load(f\"data/{noise_rate}/single_labels_labeled.npy\")\n",
    "\n",
    "    true_labels_labeled = np.load(f\"data/{noise_rate}/true_labels_labeled.npy\")\n",
    "    true_labels_unlabeled = np.load(f\"data/{noise_rate}/true_labels_unlabeled.npy\")\n",
    "    true_labels_test = np.load(f\"data/{noise_rate}/true_labels_test.npy\")\n",
    "\n",
    "    extra_labels_labeled = np.load(f\"data/{noise_rate}/extra_labels_labeled.npy\")\n",
    "    extra_labels_unlabeled = np.load(f\"data/{noise_rate}/extra_labels_unlabeled.npy\")\n",
    "\n",
    "    X_labeled = np.load(f\"data/{noise_rate}/X_labeled.npy\")\n",
    "    X_unlabeled = np.load(f\"data/{noise_rate}/X_unlabeled.npy\")\n",
    "    X_test = np.load(f\"data/{noise_rate}/X_test.npy\")\n",
    "\n",
    "    return (\n",
    "        multiannotator_labels,\n",
    "        single_labels,\n",
    "        X_labeled,\n",
    "        X_unlabeled,\n",
    "        X_test,\n",
    "        true_labels_labeled,\n",
    "        true_labels_unlabeled,\n",
    "        true_labels_test,\n",
    "        extra_labels_labeled,\n",
    "        extra_labels_unlabeled,\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise_rate in noise_rate_arr:\n",
    "    for iter in range(num_iter):\n",
    "        dirname = f\"results/{noise_rate}\"\n",
    "        !mkdir -p $dirname\n",
    "\n",
    "        model_accuracy_arr = np.full(num_rounds, np.nan)\n",
    "\n",
    "        (multiannotator_labels, single_labels, X_labeled, X_unlabeled, X_test,\n",
    "        y_labeled, y_unlabeled, y_test, extra_labels_labeled, \n",
    "        extra_labels_unlabeled) = get_data(noise_rate)\n",
    "\n",
    "        for i in range(num_rounds):\n",
    "            # get consensus labels\n",
    "            if i == 0:\n",
    "                consensus_labels = get_majority_vote_label(multiannotator_labels)\n",
    "            else:\n",
    "                # we can use the pred_probs from last round as the best model estimate\n",
    "                results = get_label_quality_multiannotator(\n",
    "                    multiannotator_labels, \n",
    "                    pred_probs_labeled,\n",
    "                    calibrate_probs=True,\n",
    "                )\n",
    "                consensus_labels = results[\"label_quality\"][\"consensus_label\"]\n",
    "\n",
    "            # train model to get out-of-sample predicted probabilites \n",
    "            pred_probs, pred_probs_unlabeled = fit_predict_proba(\n",
    "                ExtraTreesClassifier(),\n",
    "                X_labeled,\n",
    "                consensus_labels,\n",
    "                cv_n_folds=5,\n",
    "                X_unlabeled=X_unlabeled,\n",
    "            )\n",
    "\n",
    "            # train a model on the full set of labeled data to evaluate model accuracy for the current round,\n",
    "            # this is an optional step for demonstration purposes, in practical applications \n",
    "            # you may not have ground truth labels\n",
    "            model = ExtraTreesClassifier()\n",
    "            model.fit(X_labeled, consensus_labels)\n",
    "            pred_labels = model.predict(X_test)\n",
    "            model_accuracy_arr[i] = np.mean(pred_labels == y_test)\n",
    "\n",
    "            # compute active learning scores\n",
    "            active_learning_scores, active_learning_scores_unlabeled = get_active_learning_scores(\n",
    "                multiannotator_labels, pred_probs, pred_probs_unlabeled\n",
    "            )\n",
    "\n",
    "            # get the indices of examples to collect more labels for\n",
    "            relabel_idx, relabel_idx_unlabeled = get_idx_to_label(\n",
    "                active_learning_scores=active_learning_scores,\n",
    "                active_learning_scores_unlabeled=active_learning_scores_unlabeled,\n",
    "                batch_size_to_label=batch_size_to_label,\n",
    "            )\n",
    "\n",
    "            # format the data for the next round of active learning, ie. moving some unlabeled \n",
    "            # examples to the labeled pool because we are collecting labels for them\n",
    "            (\n",
    "                multiannotator_labels, relabel_idx_combined, X_labeled, X_unlabeled, pred_probs_labeled, \n",
    "                pred_probs_unlabeled, extra_labels_labeled, extra_labels_unlabeled,\n",
    "            ) = setup_next_iter_data(\n",
    "                multiannotator_labels, relabel_idx, relabel_idx_unlabeled, X_labeled, X_unlabeled, pred_probs, \n",
    "                pred_probs_unlabeled, extra_labels_labeled, extra_labels_unlabeled,\n",
    "            )\n",
    "\n",
    "            # add a new annotator that provides new labels for the examples with the lowest\n",
    "            # active learning scores (indices obtained above)\n",
    "            multiannotator_labels = add_new_annotator(\n",
    "                multiannotator_labels, extra_labels_labeled, relabel_idx_combined\n",
    "            )\n",
    "\n",
    "        np.save(f\"results/{noise_rate}/multiannotator_{iter}.npy\", model_accuracy_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise_rate in noise_rate_arr:\n",
    "    for iter in range(num_iter):\n",
    "        dirname = f\"results/{noise_rate}\"\n",
    "        !mkdir -p $dirname\n",
    "\n",
    "        single_model_accuracy_arr = np.full(num_rounds, np.nan)\n",
    "\n",
    "        (multiannotator_labels, single_labels, X_labeled, X_unlabeled, X_test,\n",
    "        y_labeled, y_unlabeled, y_test, _, extra_labels_single) = get_data(noise_rate)\n",
    "\n",
    "        for i in range(num_rounds):\n",
    "            # train model to get out-of-sample predicted probabilites \n",
    "            pred_probs, pred_probs_unlabeled = fit_predict_proba(\n",
    "                ExtraTreesClassifier(),\n",
    "                X_labeled,\n",
    "                single_labels,\n",
    "                cv_n_folds=5,\n",
    "                X_unlabeled=X_unlabeled,\n",
    "            )\n",
    "\n",
    "            # train a model on the full set of labeled data to evaluate model accuracy for the current round,\n",
    "            # this is an optional step for demonstration purposes, in practical applications \n",
    "            # you may not have ground truth labels\n",
    "            # train model on single label\n",
    "            single_model = ExtraTreesClassifier()\n",
    "            single_model.fit(X_labeled, single_labels)\n",
    "            single_pred_labels = single_model.predict(X_test)\n",
    "            single_model_accuracy_arr[i] = np.mean(single_pred_labels == y_test)\n",
    "\n",
    "            # compute active learning scores (entropy)\n",
    "            quality_of_consensus = - get_normalized_entropy(pred_probs_unlabeled)\n",
    "\n",
    "            # get the indices of examples to collect more labels for\n",
    "            relabel_idx = np.array([])\n",
    "            relabel_idx_unlabeled = np.argsort(quality_of_consensus)[:batch_size_to_label]\n",
    "\n",
    "            # add a new annotator that provides new labels for the examples with the lowest\n",
    "            # active learning scores (indices obtained above)\n",
    "            new_single_annotator_labels = add_new_annotator_single(extra_labels_single, relabel_idx_unlabeled)\n",
    "            single_labels = np.concatenate((single_labels, new_single_annotator_labels))\n",
    "\n",
    "            # format the data for the next round of active learning, ie. moving some unlabeled \n",
    "            # examples to the labeled pool because we are collecting labels for them\n",
    "            (\n",
    "                relabel_idx_combined, X_labeled, X_unlabeled, pred_probs_labeled, \n",
    "                pred_probs_unlabeled, extra_labels_single\n",
    "            ) = setup_next_iter_data_single(\n",
    "                relabel_idx, relabel_idx_unlabeled, X_labeled, X_unlabeled, pred_probs, \n",
    "                pred_probs_unlabeled, extra_labels_single\n",
    "            )\n",
    "\n",
    "            np.save(f\"results/{noise_rate}/single_{iter}.npy\", single_model_accuracy_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
