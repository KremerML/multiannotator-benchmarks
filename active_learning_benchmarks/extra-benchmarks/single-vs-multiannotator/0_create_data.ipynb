{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cleanlab.benchmarking.noise_generation import (\n",
    "    generate_noise_matrix_from_trace,\n",
    "    generate_noisy_labels,\n",
    ")\n",
    "from cleanlab.multiannotator import get_majority_vote_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synthetic_multiannotator_labels(\n",
    "    true_labels,\n",
    "    noise_rate=0.85,\n",
    "    num_annotators=30,\n",
    "):\n",
    "    n = len(true_labels)\n",
    "    m = len(np.unique(true_labels))  # num classes\n",
    "    py = np.bincount(true_labels) / float(len(true_labels))\n",
    "\n",
    "    noise_matrix = generate_noise_matrix_from_trace(\n",
    "        m,\n",
    "        trace=noise_rate * m,\n",
    "        py=py,\n",
    "        valid_noise_matrix=True,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    multiannotator_labels = np.vstack(\n",
    "        [\n",
    "            generate_noisy_labels(true_labels, noise_matrix)\n",
    "            for i in range(num_annotators)\n",
    "        ]\n",
    "    ).transpose()\n",
    "\n",
    "    return multiannotator_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synthetic_labels(\n",
    "    true_labels,\n",
    "    noise_rate=0.85,\n",
    "):\n",
    "    n = len(true_labels)\n",
    "    m = len(np.unique(true_labels))  # num classes\n",
    "    py = np.bincount(true_labels) / float(len(true_labels))\n",
    "\n",
    "    noise_matrix = generate_noise_matrix_from_trace(\n",
    "        m,\n",
    "        trace=noise_rate * m,\n",
    "        py=py,\n",
    "        valid_noise_matrix=True,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    labels = generate_noisy_labels(true_labels, noise_matrix)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns sample labels/error_mask/annotator_mask where x_drop, y_drop are idxs that are dropped\n",
    "def get_sample_labels(x_drop, y_drop, labels, annotator_mask):\n",
    "    s_annotator_mask = annotator_mask.copy()\n",
    "    s_annotator_mask[(x_drop, y_drop)] = 0\n",
    "    s_labels = labels.copy().astype(\"float64\")\n",
    "    np.copyto(s_labels, np.nan, where=(s_annotator_mask == 0))\n",
    "    # print(\"Total idxs dropped: \", annotator_mask.sum() - s_annotator_mask.sum())\n",
    "    return s_labels, s_annotator_mask\n",
    "\n",
    "\n",
    "# Returns a list of labeled indices to drop\n",
    "def get_random_drop_per_row_min_annotators(annotator_mask, max_annotations=5):\n",
    "    x, y = np.where(annotator_mask == 1)\n",
    "    xy = set([(x_idx, y_idx) for x_idx, y_idx in zip(x, y)])\n",
    "    idx_df = pd.DataFrame(zip(x, y), columns=[\"x\", \"y\"])\n",
    "    idx_keep = []\n",
    "    for x_idx in range(idx_df[\"x\"].max() + 1):\n",
    "        Y = idx_df[idx_df[\"x\"] == x_idx][\"y\"]\n",
    "        y_keep = np.random.choice(list(y), max_annotations, replace=False)\n",
    "        xy_keep = [(x_idx, y) for y in y_keep]\n",
    "        idx_keep.extend(xy_keep)\n",
    "    xy = xy.difference(set(idx_keep))\n",
    "    x_drop = [xy_idx[0] for xy_idx in xy]\n",
    "    y_drop = [xy_idx[1] for xy_idx in xy]\n",
    "    return x_drop, y_drop\n",
    "\n",
    "\n",
    "def get_random_drop_per_row(annotator_mask):\n",
    "    x, y = np.where(annotator_mask == 1)\n",
    "    idx_df = pd.DataFrame(zip(x, y), columns=[\"x\", \"y\"])\n",
    "    for x_idx in range(idx_df[\"x\"].max() + 1):\n",
    "        num_drop = np.random.randint(1, len(idx_df[idx_df[\"x\"] == x_idx]) + 1)\n",
    "        idx_df = idx_df.drop(idx_df[idx_df[\"x\"] == x_idx].sample(num_drop).index)\n",
    "    x_drop = idx_df[\"x\"].values\n",
    "    y_drop = idx_df[\"y\"].values\n",
    "    return x_drop, y_drop\n",
    "\n",
    "\n",
    "def get_least_annotations(annotator_mask):\n",
    "    annotations_per_example = annotator_mask.sum(axis=1)\n",
    "    annotations_per_annotator = annotator_mask.sum(axis=0)\n",
    "\n",
    "    temp_mask = annotator_mask.copy()\n",
    "\n",
    "    for ex in range(temp_mask.shape[0]):\n",
    "        if (\n",
    "            annotations_per_example[ex] < 2\n",
    "        ):  # ignore dropping when there are very little annotations\n",
    "            continue\n",
    "\n",
    "        annotators = np.where(temp_mask[ex] == 1)[0]  # annotators for example\n",
    "        drop_y = np.random.choice(annotators, len(annotators) - 1, replace=False)\n",
    "\n",
    "        for y in drop_y:\n",
    "            if np.random.uniform() > 0.03:\n",
    "                x = np.where(temp_mask[:, y] == 1)[\n",
    "                    0\n",
    "                ]  # annotations for annotator y for all examples\n",
    "                x = np.setdiff1d(\n",
    "                    x, np.array([ex])\n",
    "                )  # annotations for annotator y for all examples minus curent example\n",
    "                if (\n",
    "                    annotations_per_example[x].max() > 2\n",
    "                ):  # number of total annotations by our annotator\n",
    "                    temp_mask[ex][y] = 0\n",
    "                    annotations_per_annotator[y] -= 1\n",
    "                    annotations_per_example[ex] -= 1\n",
    "\n",
    "    x_drop, y_drop = np.where(temp_mask == 0)\n",
    "\n",
    "    return x_drop, y_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(noise_rate):\n",
    "    wallrobot = pd.read_csv(\"data/wall_robot_subset.csv\", index_col=0)\n",
    "    num_annotators = 30\n",
    "\n",
    "    X = wallrobot.loc[:, wallrobot.columns != \"class\"].to_numpy()\n",
    "    y = wallrobot[\"class\"].to_numpy()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=1 / 3, random_state=SEED\n",
    "    )\n",
    "    multiannotator_labels = get_synthetic_multiannotator_labels(\n",
    "        y_train, noise_rate=noise_rate, num_annotators=num_annotators\n",
    "    )\n",
    "    extra_multiannotator_labels = get_synthetic_multiannotator_labels(\n",
    "        y_train, noise_rate=noise_rate, num_annotators=num_annotators\n",
    "    )\n",
    "\n",
    "    consensus_label = get_majority_vote_label(multiannotator_labels)\n",
    "\n",
    "    single_labels = get_synthetic_labels(y_train, noise_rate=noise_rate)\n",
    "    extra_single_labels = get_synthetic_labels(y_train, noise_rate=noise_rate)\n",
    "\n",
    "    (\n",
    "        X_labeled,\n",
    "        X_unlabeled,\n",
    "        y_labeled,\n",
    "        y_unlabeled,\n",
    "        multiannotator_labeled,\n",
    "        multiannotator_unlabeled,\n",
    "        single_labeled,\n",
    "        single_unlabeled,\n",
    "    ) = train_test_split(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        multiannotator_labels,\n",
    "        single_labels,\n",
    "        test_size=0.75,\n",
    "        random_state=SEED,\n",
    "    )\n",
    "\n",
    "    annotator_mask = np.full(multiannotator_labeled.shape, 1)\n",
    "\n",
    "    x_drop, y_drop = get_least_annotations(annotator_mask)\n",
    "    multiannotator_labels, annotator_mask = get_sample_labels(\n",
    "        x_drop, y_drop, multiannotator_labeled, annotator_mask\n",
    "    )\n",
    "\n",
    "    consensus_label = get_majority_vote_label(multiannotator_labels)\n",
    "    single_labeled = consensus_label\n",
    "\n",
    "    # print(pd.DataFrame(multiannotator_labels).count(axis=1).value_counts())\n",
    "\n",
    "    # Check if dataset creation conditions are met\n",
    "    for col in range(annotator_mask.shape[1]):\n",
    "        annotator = np.where(annotator_mask[:, col] == 1)[0]\n",
    "        intersects = False\n",
    "        for j in range(annotator_mask.shape[1]):\n",
    "            if j == col:\n",
    "                continue\n",
    "            annotator2 = np.where(annotator_mask[:, j] == 1)[0]\n",
    "            if len(np.intersect1d(annotator, annotator2)) > 0:\n",
    "                intersects = True\n",
    "        if not intersects:\n",
    "            print(f\"annotator {col} does not intersect with any other annotator\")\n",
    "\n",
    "    anno_acc = pd.DataFrame(multiannotator_labels).apply(\n",
    "        lambda s: np.mean(s[pd.notna(s)] == y_labeled[pd.notna(s)])\n",
    "    )\n",
    "    print(f\"min accuracy = {np.min(anno_acc)}\")\n",
    "    print(f\"max accuracy = {np.max(anno_acc)}\")\n",
    "    print(f\"avg accuracy = {np.mean(anno_acc)}\")\n",
    "\n",
    "    consensus_label = get_majority_vote_label(multiannotator_labels)\n",
    "    accuracy = np.mean(consensus_label == y_labeled)\n",
    "    print(f\"base consensus accuracy = {accuracy}\")\n",
    "\n",
    "    accuracy = np.mean(\n",
    "        pd.DataFrame(multiannotator_unlabeled).apply(\n",
    "            lambda s: np.mean(s == y_unlabeled)\n",
    "        )\n",
    "    )\n",
    "    print(f\"base extra label accuracy = {accuracy}\")\n",
    "\n",
    "    accuracy = np.mean(single_labeled == y_labeled)\n",
    "    print(f\"base single label accuracy = {accuracy}\")\n",
    "\n",
    "    # accuracy = np.mean(single_unlabeled == y_unlabeled)\n",
    "    # print(f\"base extra single label accuracy = {accuracy}\")\n",
    "\n",
    "    extra_labels_labeled = multiannotator_labeled\n",
    "    extra_labels_unlabeled = multiannotator_unlabeled\n",
    "\n",
    "    return (\n",
    "        X_labeled,\n",
    "        X_unlabeled,\n",
    "        X_test,\n",
    "        y_labeled,\n",
    "        y_unlabeled,\n",
    "        y_test,\n",
    "        multiannotator_labels,\n",
    "        single_labeled,\n",
    "        extra_labels_labeled,\n",
    "        extra_labels_unlabeled,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min accuracy = 0.5384615384615384\n",
      "max accuracy = 0.8\n",
      "avg accuracy = 0.6739903977817376\n",
      "base consensus accuracy = 0.71\n",
      "base extra label accuracy = 0.6675555555555556\n",
      "base single label accuracy = 0.71\n",
      "\n",
      "min accuracy = 0.5769230769230769\n",
      "max accuracy = 0.9\n",
      "avg accuracy = 0.7648264686582015\n",
      "base consensus accuracy = 0.818\n",
      "base extra label accuracy = 0.7662666666666667\n",
      "base single label accuracy = 0.818\n",
      "\n",
      "min accuracy = 0.6923076923076923\n",
      "max accuracy = 0.9655172413793104\n",
      "avg accuracy = 0.8665210294828466\n",
      "base consensus accuracy = 0.896\n",
      "base extra label accuracy = 0.8635333333333332\n",
      "base single label accuracy = 0.896\n",
      "\n",
      "min accuracy = 0.7586206896551724\n",
      "max accuracy = 1.0\n",
      "avg accuracy = 0.9373028852681975\n",
      "base consensus accuracy = 0.95\n",
      "base extra label accuracy = 0.9301333333333335\n",
      "base single label accuracy = 0.95\n",
      "\n",
      "min accuracy = 1.0\n",
      "max accuracy = 1.0\n",
      "avg accuracy = 1.0\n",
      "base consensus accuracy = 1.0\n",
      "base extra label accuracy = 1.0\n",
      "base single label accuracy = 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for noise_rate in [0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "    \n",
    "    dirname = f\"data/{noise_rate}\"\n",
    "    !mkdir -p $dirname\n",
    "\n",
    "    (\n",
    "        X_labeled,\n",
    "        X_unlabeled,\n",
    "        X_test,\n",
    "        y_labeled,\n",
    "        y_unlabeled,\n",
    "        y_test,\n",
    "        multiannotator_labels,\n",
    "        single_labeled,\n",
    "        extra_labels_labeled,\n",
    "        extra_labels_unlabeled,\n",
    "    ) = create_data(noise_rate)\n",
    "\n",
    "    np.save(f\"data/{noise_rate}/X_labeled.npy\", X_labeled)\n",
    "    np.save(f\"data/{noise_rate}/X_unlabeled.npy\", X_unlabeled)\n",
    "    np.save(f\"data/{noise_rate}/X_test.npy\", X_test)\n",
    "\n",
    "    np.save(f\"data/{noise_rate}/true_labels_labeled.npy\", y_labeled)\n",
    "    np.save(f\"data/{noise_rate}/true_labels_unlabeled.npy\", y_unlabeled)\n",
    "    np.save(f\"data/{noise_rate}/true_labels_test.npy\", y_test)\n",
    "\n",
    "    np.save(f\"data/{noise_rate}/multiannotator_labels_labeled.npy\", multiannotator_labels)\n",
    "    np.save(f\"data/{noise_rate}/single_labels_labeled.npy\", single_labeled)\n",
    "    np.save(f\"data/{noise_rate}/extra_labels_labeled.npy\", extra_labels_labeled)\n",
    "    np.save(f\"data/{noise_rate}/extra_labels_unlabeled.npy\", extra_labels_unlabeled)\n",
    "\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 21 2022, 22:22:30) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
